{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, \\\n",
    "f1_score\n",
    "\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_csv('data/train.csv',parse_dates=['Original_Quote_Date'])\n",
    "test = pd.read_csv('data/test.csv',parse_dates=['Original_Quote_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset rows:\t 3500\n",
      "Validation dataset rows:\t 1500\n"
     ]
    }
   ],
   "source": [
    "# create training and test subset\n",
    "training_df, validation_df=training_validation_subset(train[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Accuracy Score: 0.8165714285714286\n"
     ]
    }
   ],
   "source": [
    "# Null Accuracy Score\n",
    "y_train=training_df['QuoteConversion_Flag']\n",
    "lst = [0] * len(y_train)\n",
    "print(f'Null Accuracy Score: {accuracy_score(y_train, pd.Series(lst))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoder(training_df, validation_df, test_df, col, response):\n",
    "\n",
    "    \"\"\" function to mean encode categorical features \n",
    "        any missing values are imputed with mode \"\"\"\n",
    "\n",
    "    # Create dictionary\n",
    "    mean_encoding=training_df.groupby(col)[response].mean().to_dict()\n",
    "\n",
    "    # Apply to train\n",
    "    training_df[col+'_ME']=training_df[col].replace(mean_encoding)\n",
    "    training_df[col+'_ME']=training_df[col+'_ME'].fillna(training_df[col+'_ME'].mode()[0])\n",
    "\n",
    "    # Apply to valid\n",
    "    validation_df[col+'_ME']=validation_df[col].replace(mean_encoding)\n",
    "    validation_df[col+'_ME']=validation_df[col+'_ME'].fillna(validation_df[col+'_ME'].mode()[0])\n",
    "\n",
    "    # Apply to test\n",
    "    test_df[col+'_ME']=test_df[col].replace(mean_encoding)\n",
    "    test_df[col+'_ME']=test_df[col+'_ME'].fillna(test_df[col+'_ME'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_outlier_capping(df, variable, multiplier):\n",
    "\n",
    "    ''' cap and collar the response variable '''\n",
    "\n",
    "    q1 = np.percentile(df[variable],25)\n",
    "    q3 = np.percentile(df[variable],75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - (iqr * multiplier)\n",
    "    upper = q3 + (iqr * multiplier)\n",
    "\n",
    "    df[variable] = np.where(df[variable]<=lower, lower, df[variable])\n",
    "    df[variable] = np.where(df[variable]>=upper, upper, df[variable])\n",
    "\n",
    "    return df\n",
    "\n",
    "# numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'uint8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PersonalField18_ME\n",
      "[0.15162455 0.19047619 0.23574144 0.17073171 0.27835052 0.22222222\n",
      " 0.1        0.22972973 0.25358852 0.18137255 0.2244898  0.23076923\n",
      " 0.25       0.11764706 0.13157895 0.31914894 0.2        0.2173913\n",
      " 0.29411765 0.375      0.09090909 0.09375    0.2962963  0.14285714\n",
      " 0.11111111 0.17647059 0.0625     0.         0.16666667 0.15\n",
      " 0.14705882 0.21428571 0.5        0.27272727 0.28571429 0.33333333]\n",
      "PersonalField18 transformed...\n",
      "\n",
      "Field10_ME\n",
      "[0.16466552 0.32397959 0.22543353 0.08485857 0.28219697 0.05925926\n",
      " 0.11560694 0.10344828]\n",
      "Field10 transformed...\n",
      "\n",
      "PersonalField19_ME\n",
      "[0.15162455 0.24020619 0.         0.20555556 0.10344828 0.125\n",
      " 0.11940299 0.3        0.2        0.07692308 0.1        0.30769231\n",
      " 0.22727273 0.07142857 0.4        0.18181818 0.16129032 0.16\n",
      " 0.13043478 0.22222222 0.09090909 0.25       0.17647059 0.05555556\n",
      " 0.66666667 0.5        0.33333333]\n",
      "PersonalField19 transformed...\n",
      "\n",
      "PropertyField38_ME\n",
      "[0.18552169 0.12857143]\n",
      "PropertyField38 transformed...\n",
      "\n",
      "CoverageField9_ME\n",
      "[0.05082212 0.32352941 0.08991228 0.23076923 0.06430155 0.26923077\n",
      " 0.33333333 0.11764706 0.11111111 0.        ]\n",
      "CoverageField9 transformed...\n",
      "\n",
      "PropertyField14_ME\n",
      "[0.16932422 0.21721311 0.23809524]\n",
      "PropertyField14 transformed...\n",
      "\n",
      "PropertyField3_ME\n",
      "[0.18425351 0.17767654]\n",
      "PropertyField3 transformed...\n",
      "\n",
      "PropertyField31_ME\n",
      "[0.19530876 0.12814645 0.18928571 0.14179104]\n",
      "PropertyField31 transformed...\n",
      "\n",
      "PropertyField33_ME\n",
      "[0.22824791 0.10364146 0.20344288 0.16071429]\n",
      "PropertyField33 transformed...\n",
      "\n",
      "SalesField7_ME\n",
      "[0.20689655 0.18991098 0.20758123 0.18823529 0.19633943 0.13194444\n",
      " 0.06190476]\n",
      "SalesField7 transformed...\n",
      "\n",
      "CoverageField8_ME\n",
      "[0.27272727 0.10447761 0.22175512 0.         0.42857143 0.16666667\n",
      " 0.25      ]\n",
      "CoverageField8 transformed...\n",
      "\n",
      "Field12_ME\n",
      "[0.19021237 0.09561753]\n",
      "Field12 transformed...\n",
      "\n",
      "GeographicField63_ME\n",
      "[0.18468732 0.12820513]\n",
      "GeographicField63 transformed...\n",
      "\n",
      "PersonalField7_ME\n",
      "[0.18414249 0.05882353]\n",
      "PersonalField7 transformed...\n",
      "\n",
      "PropertyField7_ME\n",
      "[0.19837233 0.27624309 0.21695761 0.11842105 0.0956341  0.22983425\n",
      " 0.12612613 0.05957447 0.11111111 0.15384615 0.29411765 0.14285714\n",
      " 0.         0.5       ]\n",
      "PropertyField7 transformed...\n",
      "\n",
      "PropertyField34_ME\n",
      "[0.25945106 0.08992347]\n",
      "PropertyField34 transformed...\n",
      "\n",
      "PersonalField16_ME\n",
      "[0.15162455 0.07692308 0.16455696 0.19444444 0.29032258 0.1686747\n",
      " 0.16438356 0.28767123 0.26136364 0.15873016 0.28888889 0.19047619\n",
      " 0.24793388 0.20689655 0.31707317 0.4        0.29166667 0.14864865\n",
      " 0.09090909 0.21100917 0.19607843 0.17045455 0.24186047 0.\n",
      " 0.22580645 0.2        0.22222222 0.14285714 0.17142857 0.25\n",
      " 0.06666667 0.6        0.16666667 1.        ]\n",
      "PersonalField16 transformed...\n",
      "\n",
      "PropertyField28_ME\n",
      "[0.18665019 0.16       0.14492754 0.07692308]\n",
      "PropertyField28 transformed...\n",
      "\n",
      "PropertyField37_ME\n",
      "[0.15672235 0.2515213 ]\n",
      "PropertyField37 transformed...\n",
      "\n",
      "PropertyField36_ME\n",
      "[0.18570998 0.14364641]\n",
      "PropertyField36 transformed...\n",
      "\n",
      "PropertyField30_ME\n",
      "[0.19095325 0.0631068 ]\n",
      "PropertyField30 transformed...\n",
      "\n",
      "PropertyField32_ME\n",
      "[0.22658375 0.0637149 ]\n",
      "PropertyField32 transformed...\n",
      "\n",
      "GeographicField64_ME\n",
      "[0.19764706 0.3        0.07692308 0.10829493]\n",
      "GeographicField64 transformed...\n",
      "\n",
      "PersonalField17_ME\n",
      "[0.15162455 0.21376812 0.14864865 0.20512821 0.14285714 0.32727273\n",
      " 0.22222222 0.26415094 0.2628866  0.23125    0.33333333 0.31578947\n",
      " 0.17948718 0.07692308 0.13207547 0.19897959 0.08333333 0.\n",
      " 0.13636364 0.19642857 0.2        0.20930233 0.0625     0.57894737\n",
      " 0.4        0.21428571 0.1875     0.19354839 0.2173913  0.23076923\n",
      " 0.1        0.28571429 0.25      ]\n",
      "PersonalField17 transformed...\n",
      "\n",
      "PropertyField5_ME\n",
      "[0.18348099 0.        ]\n",
      "PropertyField5 transformed...\n",
      "\n",
      "Field6_ME\n",
      "[0.1978022  0.2992383  0.07768362 0.10115607 0.13636364 0.07361963\n",
      " 1.         0.        ]\n",
      "Field6 transformed...\n",
      "\n",
      "PropertyField4_ME\n",
      "[0.18477191 0.17439294]\n",
      "PropertyField4 transformed...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_cols=return_categoric_columns(training_df)\n",
    "cat_cols.remove('Original_Quote_Date')\n",
    "\n",
    "for i in cat_cols:\n",
    "    \n",
    "    print(i+'_ME')\n",
    "    mean_encoder(training_df, validation_df, test, i, 'QuoteConversion_Flag')\n",
    "    print(training_df[i+'_ME'].unique())\n",
    "    print(i, 'transformed...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "# numeric and categoric columns in train\n",
    "train_numeric_predictors = train.select_dtypes(include=numerics).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=training_df.drop(['QuoteNumber','Original_Quote_Date','QuoteConversion_Flag'], axis=1)\n",
    "# Y_train=training_df['QuoteConversion_Flag']\n",
    "# X_valid=validation_df.drop(['QuoteNumber','Original_Quote_Date','QuoteConversion_Flag'], axis=1)\n",
    "# Y_valid=validation_df['QuoteConversion_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=training_df[train_numeric_predictors].drop(['QuoteNumber','QuoteConversion_Flag'], axis=1)\n",
    "Y_train=training_df['QuoteConversion_Flag']\n",
    "X_valid=validation_df[train_numeric_predictors].drop(['QuoteNumber','QuoteConversion_Flag'], axis=1)\n",
    "Y_valid=validation_df['QuoteConversion_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "for i in list(X_train.columns):\n",
    "    X_train[i]=X_train[i].fillna(X_train[i].median())\n",
    "    \n",
    "for i in list(X_valid.columns):\n",
    "    X_valid[i]=X_valid[i].fillna(X_valid[i].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 85.600%\n",
      "Validation F1 Score: [0.91212368 0.60147601]\n"
     ]
    }
   ],
   "source": [
    "logistic_model=LogisticRegression(max_iter=5000)\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform train data\n",
    "train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "#Fit the model:\n",
    "logistic_model.fit(train_scaled,Y_train)\n",
    "\n",
    "# transform valid\n",
    "valid_scaled=scaler.transform(X_valid)\n",
    "\n",
    "#Make predictions on training set:\n",
    "predictions = logistic_model.predict(valid_scaled)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train and valid\n",
    "training_df['logistic_prediction'] = logistic_model.predict_proba(train_scaled)[:, 1]\n",
    "validation_df['logistic_prediction'] = logistic_model.predict_proba(valid_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 85.867%\n",
      "Validation F1 Score: [0.91403082 0.60299625]\n"
     ]
    }
   ],
   "source": [
    "elastic_net_model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5, max_iter=5000)\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform train data\n",
    "train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# fit the model:\n",
    "elastic_net_model.fit(train_scaled,Y_train)\n",
    "\n",
    "# transform valid\n",
    "valid_scaled=scaler.transform(X_valid)\n",
    "\n",
    "# make predictions on training set:\n",
    "predictions = elastic_net_model.predict(valid_scaled)\n",
    "\n",
    "# print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train and valid\n",
    "training_df['elastic_prediction'] = elastic_net_model.predict_proba(train_scaled)[:, 1]\n",
    "validation_df['elastic_prediction'] = elastic_net_model.predict_proba(valid_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 84.867%\n",
      "Validation F1 Score: [0.90761091 0.58195212]\n"
     ]
    }
   ],
   "source": [
    "# instantiate classifier with default hyperparameters with kernel=rbf, C=1.0 and gamma=auto\n",
    "svc=SVC(probability=True)\n",
    "\n",
    "# declare parameters for hyperparameter tuning\n",
    "parameters = [ {'C':[1, 10], 'kernel':['linear']},\n",
    "              ]\n",
    "\n",
    "# # declare parameters for hyperparameter tuning\n",
    "# parameters = [ {'C':[1, 10, 100, 1000], 'kernel':['linear']},\n",
    "#                {'C':[1, 10, 100, 1000], 'kernel':['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},\n",
    "#                {'C':[1, 10, 100, 1000], 'kernel':['poly'], 'degree': [2,3,4] ,'gamma':[0.01,0.02,0.03,0.04,0.05]} \n",
    "#               ]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = svc,  \n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform train data\n",
    "train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "grid_search.fit(train_scaled, Y_train)\n",
    "             \n",
    "# transform valid\n",
    "valid_scaled=scaler.transform(X_valid)\n",
    "\n",
    "# make predictions on training set:\n",
    "predictions = grid_search.predict(valid_scaled)\n",
    "\n",
    "# print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train and valid\n",
    "training_df['svm_prediction'] = grid_search.predict_proba(train_scaled)[:, 1]\n",
    "validation_df['svm_prediction'] = grid_search.predict_proba(valid_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 80.867%\n",
      "Validation F1 Score: [0.88096226 0.51273345]\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "#Fit the model:\n",
    "tree_model.fit(X_train,Y_train)\n",
    "\n",
    "#Make predictions on training set\n",
    "predictions = tree_model.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train and valid\n",
    "training_df['tree_prediction'] = tree_model.predict_proba(X_train)[:, 1]\n",
    "validation_df['tree_prediction'] = tree_model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all the features of the nucleus\n",
    "rf_model = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\n",
    "\n",
    "#Fit the model:\n",
    "rf_model.fit(X_train,Y_train)\n",
    "\n",
    "#Make predictions on training set\n",
    "predictions = rf_model.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the features improves the prediction accuracy and the cross-validation score is great.\n",
    "\n",
    "An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a series with feature importances\n",
    "featimp = pd.Series(rf_model.feature_importances_, index=list(X_train.columns)).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using top 10 features\n",
    "predictor_var = list(featimp[:10].keys())\n",
    "rf2_model = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)\n",
    "\n",
    "#Fit the model:\n",
    "rf2_model.fit(X_train[predictor_var],Y_train)\n",
    "\n",
    "#Make predictions on training set\n",
    "predictions = rf2_model.predict(X_valid[predictor_var])\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train and valid\n",
    "training_df['rf_prediction'] = rf_model.predict_proba(X_train)[:, 1]\n",
    "validation_df['rf_prediction'] = rf_model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB (Hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "ratio = float(np.sum(training_df['QuoteConversion_Flag'] == 0)) / np.sum(training_df['QuoteConversion_Flag']==1)\n",
    "scale_pos_weight_val=round(ratio,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameter search space\n",
    "space = {\n",
    "        'max_depth':hp.choice('max_depth', np.arange(2, 25, 1, dtype=int)),\n",
    "        'n_estimators':hp.choice('n_estimators', np.arange(50, 12000, 10, dtype=int)),\n",
    "        'colsample_bytree':hp.quniform('colsample_bytree', 0.4, 0.9, 0.1),\n",
    "        'min_child_weight':hp.choice('min_child_weight', np.arange(1, 12, 1, dtype=int)),\n",
    "        'scale_pos_weight':hp.choice('scale_pos_weight', np.arange(1, scale_pos_weight_val, 1, dtype=int)),   \n",
    "        'lambda':hp.choice('lambda', np.arange(1, 5, 1, dtype=int)),    \n",
    "        'subsample':hp.quniform('subsample', 0.6, 1.0, 0.1),\n",
    "        'gamma':hp.quniform('gamm', 0, 10, 1),\n",
    "        'objective':'binary:logistic',\n",
    "        'eta':hp.quniform('eta', 0.01, 0.5, 0.1),\n",
    "        'eval_metric': 'auc',\n",
    "    }\n",
    "\n",
    "def score(params):\n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    model.fit(X_train, \n",
    "              Y_train, \n",
    "              eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "              verbose=False, \n",
    "              early_stopping_rounds=25)\n",
    "    Y_pred_train = model.predict(X_train)\n",
    "    Y_pred_test = model.predict(X_valid)\n",
    "    train_score = (-1*roc_auc_score(Y_train, Y_pred_train))\n",
    "    test_score = (-1*roc_auc_score(Y_valid, Y_pred_test))\n",
    "    print('Training loss:', round(abs(train_score),4), 'Test loss:', round(abs(test_score),4))\n",
    "    return {'loss': test_score, 'status': STATUS_OK}  \n",
    "\n",
    "def optimize(trials, space):\n",
    "    \n",
    "    best = fmin(score, space, algo=tpe.suggest, max_evals=25) # up this amount\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise\n",
    "trials = Trials()\n",
    "best_params = optimize(trials, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the best parameters\n",
    "parameters=space_eval(space, best_params)\n",
    "\n",
    "# Apply to model\n",
    "xgb_model = XGBClassifier(colsample_bytree=parameters['colsample_bytree'],\n",
    " eta=parameters['eta'],\n",
    " eval_metric=parameters['eval_metric'],\n",
    " gamma=parameters['gamma'],\n",
    " max_depth=parameters['max_depth'],\n",
    " min_child_weight=parameters['min_child_weight'],\n",
    " n_estimators=parameters['n_estimators'],\n",
    " scale_pos_weight=parameters['scale_pos_weight'],\n",
    " objective=parameters['objective'],\n",
    " subsample=parameters['subsample'])\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    eval_metric=\"auc\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on training set\n",
    "predictions = xgb_model.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train and valid\n",
    "training_df['xgbho_prediction'] = xgb_model.predict_proba(X_train)[:, 1]\n",
    "validation_df['xgbho_prediction'] = xgb_model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB (Skopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS - CHANGE THESE TO GET SOMETHING MEANINGFUL\n",
    "ITERATIONS = 25 # 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'auc',\n",
    "#         early_stopping_rounds=2,\n",
    "#         silent=1,\n",
    "        tree_method='approx'\n",
    "    ),\n",
    "    search_spaces = {\n",
    "         'learning_rate': (0.001, 0.3, 'uniform'),\n",
    "         'min_child_weight': (0, 10),\n",
    "         'max_depth': (2, 30),\n",
    "#         'max_delta_step': (0, 20),\n",
    "         'subsample': (0.5, 1.0, 'uniform'),\n",
    "#         'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "#         'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "#         'reg_lambda': (1e-5, 1000, 'uniform'),\n",
    "        'reg_alpha': (1e-5, 1.0, 'uniform'),\n",
    "        'gamma': (1e-9, 0.5, 'uniform'),\n",
    "        'n_estimators': (2, 100),\n",
    "        'scale_pos_weight': (1, 500, 'uniform')\n",
    "    },\n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 45\n",
    ")\n",
    "\n",
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
    "    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner.best_score_, 4),\n",
    "        bayes_cv_tuner.best_params_\n",
    "    ))\n",
    "    \n",
    "    # Save all model results\n",
    "    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n",
    "#     all_models.to_csv(clf_name+\"_cv_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "xgb_skopt = bayes_cv_tuner.fit(X_train, Y_train, callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on training set\n",
    "predictions = xgb_skopt.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train and valid\n",
    "training_df['xgbsko_prediction'] = xgb_skopt.predict_proba(X_train)[:, 1]\n",
    "validation_df['xgbsko_prediction'] = xgb_skopt.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB (Ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_cv_score_ax(parameterization, weight=None):\n",
    "    NFOLD = 7\n",
    "    NUM_BOOST_ROUND = 500\n",
    "\n",
    "    p_names = ['learning_rate', 'max_depth' 'subsample', 'min_split_loss', 'min_child_weight', 'colsample_bytree', \n",
    "              'colsample_bylevel', 'colsample_bynode', 'lambda', 'alpha']\n",
    "    params = {}\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    \n",
    "    for p in p_names:\n",
    "        params[p] = parameterization.get(p)\n",
    "    \n",
    "    # K-Fold cross validation score.\n",
    "    cv_results = xgb.cv(dtrain=dtrain,\n",
    "                        params=params,\n",
    "                        nfold=NFOLD,\n",
    "                        num_boost_round=NUM_BOOST_ROUND,\n",
    "                        metrics=\"auc\", \n",
    "                        as_pandas=True,\n",
    "                        seed=987)\n",
    "#     print(cv_results)\n",
    "    \n",
    "    mean = cv_results.tail(1)['test-auc-mean'].values[0]\n",
    "    sem = cv_results.tail(1)['test-auc-std'].values[0]\n",
    "    \n",
    "    return mean, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster\n",
    "#\n",
    "\n",
    "parameters=[\n",
    "  {\n",
    "      \"name\": \"max_depth\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [1,18],\n",
    "      \"value_type\": \"int\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"learning_rate\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [0.000001,1],\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"gamma\",\n",
    "      \"type\": \"fixed\",\n",
    "      \"value\": 0.0,\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"max_delta_step\",\n",
    "      \"type\": \"fixed\",\n",
    "      \"value\": 0.0,\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"min_child_weight\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [0,500],\n",
    "      \"value_type\": \"int\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"subsample\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [0.5,1],\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"colsample_bytree\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [0.1,1],\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "    {\n",
    "      \"name\": \"colsample_bylevel\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [0.1,1],\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"colsample_bynode\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [0.1,1],\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"reg_alpha\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [0,2000],\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"reg_lambda\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [0,300],\n",
    "      \"value_type\": \"float\"\n",
    "  },\n",
    "  {\n",
    "      \"name\": \"scale_pos_weight\",\n",
    "      \"type\": \"range\",\n",
    "      \"bounds\": [1,500],\n",
    "      \"value_type\": \"float\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import more packages\n",
    "init_notebook_plotting()\n",
    "\n",
    "ax_client = AxClient()\n",
    "\n",
    "# create the experiment.\n",
    "ax_client.create_experiment(\n",
    "    name=\"xgboost_experiment\",\n",
    "    parameters=parameters,\n",
    "    objective_name='xgboost_cv',\n",
    "    minimize=False) # false as AUC\n",
    "\n",
    "# objective name has to be xgboost_cv\n",
    "def evaluate(parameters):\n",
    "    return {\"xgboost_cv\": xgboost_cv_score_ax(parameters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many trials.\n",
    "# Uses Guassian Processes with Expected Improvement to do the trials. This seems to be the usual way people do hyperparameter tuning.\n",
    "# Can use other models/strategies, but won't be covering it here.\n",
    "\n",
    "for i in range(50):\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    \n",
    "    print(parameters, trial_index)\n",
    "    \n",
    "    # Local evaluation here can be replaced with deployment to external system.\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all the trials\n",
    "ax_client.get_trials_data_frame().sort_values('trial_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values = ax_client.get_best_parameters()\n",
    "\n",
    "# the best set of parameters\n",
    "print(best_parameters)\n",
    "\n",
    "# the best score achieved\n",
    "means, covariances = values\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_optimization_trace()) # objective_optimum is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can see contour plots for two of the features.\n",
    "\n",
    "render(ax_client.get_contour_plot(param_x=\"learning_rate\", param_y=\"max_depth\", metric_name=\"xgboost_cv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to model\n",
    "xgbax_model = XGBClassifier(max_depth=best_parameters['max_depth'],\n",
    "learning_rate=best_parameters['learning_rate'],\n",
    "min_child_weight=best_parameters['min_child_weight'],\n",
    "subsample=best_parameters['subsample'],\n",
    "colsample_bytree=best_parameters['colsample_bytree'],\n",
    "colsample_bylevel=best_parameters['colsample_bylevel'],\n",
    "colsample_bynode=best_parameters['colsample_bynode'],\n",
    "reg_alpha=best_parameters['reg_alpha'], \n",
    "reg_lambda=best_parameters['reg_lambda'],\n",
    "gamma=best_parameters['gamma'],    \n",
    "max_delta_step=best_parameters['max_delta_step'],\n",
    "scale_pos_weight=best_parameters['scale_pos_weight'],\n",
    "objective = 'binary:logistic',\n",
    "num_boost_round=5000)\n",
    "\n",
    "xgbax_model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    eval_metric=\"auc\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on training set\n",
    "predictions = xgbax_model.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on train and valid\n",
    "training_df['xgbax_prediction'] = xgbax_model.predict_proba(X_train)[:, 1]\n",
    "validation_df['xgbax_prediction'] = xgbax_model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save results to json file.\n",
    "# ax_client.save_to_json_file()\n",
    "\n",
    "# # restore the client from json file. Handy if you want to do more trials or if your computer crashed in the middle of the trials.\n",
    "# restored_ax_client = AxClient.load_from_json_file() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['logistic_prediction',\n",
    "'elastic_prediction',\n",
    "'svm_prediction',\n",
    "'tree_prediction',\n",
    "'rf_prediction',\n",
    "'xgbho_prediction',\n",
    "'xgbsko_prediction',\n",
    "'xgbax_prediction']\n",
    "\n",
    "training_df[cols].to_csv('data/all_predictions.csv', index=False)\n",
    "print('predictions saved...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the average of all predictions\n",
    "training_df['Prediction']=training_df[cols].mean(axis=1)\n",
    "validation_df['Prediction']=validation_df[cols].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal cut off\n",
    "val=optimal_cutoff(training_df['QuoteConversion_Flag'], training_df['Prediction'])\n",
    "\n",
    "# apply cut off\n",
    "validation_df['Prediction_Outcome']=np.where(validation_df['Prediction']<=val[0],0,1)  \n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(validation_df['Prediction_Outcome'],Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, validation_df['Prediction_Outcome'], average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "X_test=test[list(X_train.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(X_test.columns):\n",
    "    X_test[i]=X_test[i].fillna(X_test[i].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring\n",
    "test_scaled=scaler.transform(X_test)\n",
    "\n",
    "test['logistic_prediction'] = logistic_model.predict_proba(test_scaled)[:, 1]\n",
    "test['elastic_prediction'] = elastic_net_model.predict_proba(test_scaled)[:, 1]\n",
    "test['svm_prediction'] = grid_search.predict_proba(test_scaled)[:, 1]\n",
    "test['tree_prediction'] = tree_model.predict_proba(X_test)[:, 1]\n",
    "test['rf_prediction'] = rf_model.predict_proba(X_test)[:, 1]\n",
    "test['xgbho_prediction'] = xgb_model.predict_proba(X_test)[:, 1]\n",
    "test['xgbsko_prediction'] = xgb_skopt.predict_proba(X_test)[:, 1]\n",
    "test['xgbax_prediction'] = xgbax_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold\n",
    "test['Prediction']=test[cols].mean(axis=1)\n",
    "\n",
    "# apply cut off\n",
    "test['QuoteConversion_Flag']=np.where(test['Prediction']<=val[0],0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output\n",
    "test['QuoteConversion_Flag']=xgb_model.predict(X_test)\n",
    "test[['QuoteNumber', 'QuoteConversion_Flag']].to_csv('data/predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
