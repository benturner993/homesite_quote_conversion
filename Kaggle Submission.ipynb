{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: https://www.kaggle.com/buddhiniw/breast-cancer-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, \\\n",
    "f1_score\n",
    "\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_csv('train.csv',parse_dates=['Original_Quote_Date'])\n",
    "test = pd.read_csv('test.csv',parse_dates=['Original_Quote_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset rows:\t 182527\n",
      "Validation dataset rows:\t 78226\n"
     ]
    }
   ],
   "source": [
    "# create training and test subset\n",
    "training_df, validation_df=training_validation_subset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Accuracy Score: 0.8121866901883009\n"
     ]
    }
   ],
   "source": [
    "# Null Accuracy Score\n",
    "y_train=training_df['QuoteConversion_Flag']\n",
    "lst = [0] * len(y_train)\n",
    "print(f'Null Accuracy Score: {accuracy_score(y_train, pd.Series(lst))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "# numeric and categoric columns in train\n",
    "train_numeric_predictors = train.select_dtypes(include=numerics).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=training_df.drop(['QuoteNumber','Original_Quote_Date','QuoteConversion_Flag'], axis=1)\n",
    "# Y_train=training_df['QuoteConversion_Flag']\n",
    "# X_valid=validation_df.drop(['QuoteNumber','Original_Quote_Date','QuoteConversion_Flag'], axis=1)\n",
    "# Y_valid=validation_df['QuoteConversion_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=training_df[train_numeric_predictors].drop(['QuoteNumber','QuoteConversion_Flag'], axis=1)\n",
    "Y_train=training_df['QuoteConversion_Flag']\n",
    "X_valid=validation_df[train_numeric_predictors].drop(['QuoteNumber','QuoteConversion_Flag'], axis=1)\n",
    "Y_valid=validation_df['QuoteConversion_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "for i in list(X_train.columns):\n",
    "    X_train[i]=X_train[i].fillna(X_train[i].median())\n",
    "    \n",
    "for i in list(X_valid.columns):\n",
    "    X_valid[i]=X_valid[i].fillna(X_valid[i].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 87.867%\n",
      "Validation F1 Score: [0.92760102 0.62573445]\n"
     ]
    }
   ],
   "source": [
    "logistic_model=LogisticRegression(max_iter=5000)\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform train data\n",
    "train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "#Fit the model:\n",
    "logistic_model.fit(train_scaled,Y_train)\n",
    "\n",
    "# transform valid\n",
    "valid_scaled=scaler.transform(X_valid)\n",
    "\n",
    "#Make predictions on training set:\n",
    "predictions = logistic_model.predict(valid_scaled)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 87.874%\n",
      "Validation F1 Score: [0.92765185 0.62559204]\n"
     ]
    }
   ],
   "source": [
    "elastic_net_model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5, max_iter=5000)\n",
    "\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform train data\n",
    "train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "#Fit the model:\n",
    "elastic_net_model.fit(train_scaled,Y_train)\n",
    "\n",
    "# transform valid\n",
    "valid_scaled=scaler.transform(X_valid)\n",
    "\n",
    "#Make predictions on training set:\n",
    "predictions = elastic_net_model.predict(valid_scaled)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 85.224%\n",
      "Validation F1 Score: [0.90884573 0.61008602]\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "#Fit the model:\n",
    "tree_model.fit(X_train,Y_train)\n",
    "\n",
    "#Make predictions on training set\n",
    "predictions = tree_model.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 81.575%\n",
      "Validation F1 Score: [0.8982413  0.02700331]\n"
     ]
    }
   ],
   "source": [
    "# Use all the features of the nucleus\n",
    "rf_model = RandomForestClassifier(n_estimators=100,min_samples_split=25, max_depth=7, max_features=2)\n",
    "\n",
    "#Fit the model:\n",
    "rf_model.fit(X_train,Y_train)\n",
    "\n",
    "#Make predictions on training set\n",
    "predictions = rf_model.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the features improves the prediction accuracy and the cross-validation score is great.\n",
    "\n",
    "An advantage with Random Forest is that it returns a feature importance matrix which can be used to select features. So lets select the top 5 features and use them as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a series with feature importances\n",
    "featimp = pd.Series(rf_model.feature_importances_, index=list(X_train.columns)).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy : 86.352%\n",
      "Validation F1 Score: [0.91914326 0.56274574]\n"
     ]
    }
   ],
   "source": [
    "# Using top 10 features\n",
    "predictor_var = list(featimp[:10].keys())\n",
    "rf2_model = RandomForestClassifier(n_estimators=100, min_samples_split=25, max_depth=7, max_features=2)\n",
    "\n",
    "#Fit the model:\n",
    "rf2_model.fit(X_train[predictor_var],Y_train)\n",
    "\n",
    "#Make predictions on training set\n",
    "predictions = rf2_model.predict(X_valid[predictor_var])\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB (Hyperopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the balance of positive and negative weights, useful for unbalanced classes. \n",
    "ratio = float(np.sum(training_df['QuoteConversion_Flag'] == 0)) / np.sum(training_df['QuoteConversion_Flag']==1)\n",
    "scale_pos_weight_val=round(ratio,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-935cedac22f0>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-935cedac22f0>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    return {'loss': test_score, 'status': STATUS_OK}\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Choose hyperparameter search space\n",
    "space = {\n",
    "        'max_depth':hp.choice('max_depth', np.arange(2, 25, 1, dtype=int)),\n",
    "        'n_estimators':hp.choice('n_estimators', np.arange(50, 12000, 10, dtype=int)),\n",
    "        'colsample_bytree':hp.quniform('colsample_bytree', 0.4, 0.9, 0.1),\n",
    "        'min_child_weight':hp.choice('min_child_weight', np.arange(1, 12, 1, dtype=int)),\n",
    "        'scale_pos_weight':hp.choice('scale_pos_weight', np.arange(1, scale_pos_weight_val, 1, dtype=int)),   \n",
    "        'lambda':hp.choice('lambda', np.arange(1, 5, 1, dtype=int)),    \n",
    "        'subsample':hp.quniform('subsample', 0.6, 1.0, 0.1),\n",
    "        'gamma':hp.quniform('gamm', 0, 10, 1),\n",
    "        'objective':'binary:logistic',\n",
    "        'eta':hp.quniform('eta', 0.01, 0.5, 0.1),\n",
    "        'eval_metric': 'auc',\n",
    "    }\n",
    "\n",
    "def score(params):\n",
    "    model = XGBClassifier(**params)\n",
    "    \n",
    "    model.fit(X_train, \n",
    "              Y_train, \n",
    "              eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "              verbose=False, \n",
    "              early_stopping_rounds=25)\n",
    "    Y_pred_train = model.predict(X_train)\n",
    "    Y_pred_test = model.predict(X_valid)\n",
    "    train_score = (-1*roc_auc_score(Y_train, Y_pred_train))\n",
    "    test_score = (-1*roc_auc_score(Y_valid, Y_pred_test))\n",
    "    print('Training loss:', round(abs(train_score),2), 'Test loss:', round(abs(test_score),2)\n",
    "    return {'loss': test_score, 'status': STATUS_OK}   \n",
    "    \n",
    "    \n",
    "def optimize(trials, space):\n",
    "    \n",
    "    best = fmin(score, space, algo=tpe.suggest, max_evals=25) # up this amount\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise\n",
    "trials = Trials()\n",
    "best_params = optimize(trials, space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the best parameters\n",
    "parameters=space_eval(space, best_params)\n",
    "\n",
    "# Apply to model\n",
    "xgb_model = XGBClassifier(colsample_bytree=parameters['colsample_bytree'],\n",
    " eta=parameters['eta'],\n",
    " eval_metric=parameters['eval_metric'],\n",
    " gamma=parameters['gamma'],\n",
    " max_depth=parameters['max_depth'],\n",
    " min_child_weight=parameters['min_child_weight'],\n",
    " n_estimators=parameters['n_estimators'],\n",
    " scale_pos_weight=parameters['scale_pos_weight'],\n",
    " objective=parameters['objective'],\n",
    " subsample=parameters['subsample'])\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    eval_metric=\"auc\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on training set\n",
    "predictions = xgb_model.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test=test[train_numeric_predictors].drop(['QuoteNumber','QuoteConversion_Flag'], axis=1)\n",
    "# test['QuoteConversion_Flag']=best_model.predict(X_test)\n",
    "# test[['QuoteNumber', 'QuoteConversion_Flag']].to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB (Skopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS - CHANGE THESE TO GET SOMETHING MEANINGFUL\n",
    "ITERATIONS = 1 # 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        n_jobs = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'auc',\n",
    "#         early_stopping_rounds=2,\n",
    "#         silent=1,\n",
    "        tree_method='approx'\n",
    "    ),\n",
    "    search_spaces = {\n",
    "         'learning_rate': (0.001, 0.3, 'uniform'),\n",
    "         'min_child_weight': (0, 10),\n",
    "         'max_depth': (2, 30),\n",
    "#         'max_delta_step': (0, 20),\n",
    "         'subsample': (0.5, 1.0, 'uniform'),\n",
    "#         'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "#         'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "#         'reg_lambda': (1e-5, 1000, 'uniform'),\n",
    "        'reg_alpha': (1e-5, 1.0, 'uniform'),\n",
    "        'gamma': (1e-9, 0.5, 'uniform'),\n",
    "        'n_estimators': (2, 100),\n",
    "        'scale_pos_weight': (1, 500, 'uniform')\n",
    "    },\n",
    "    scoring = 'roc_auc',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 3,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 45\n",
    ")\n",
    "\n",
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
    "    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner.best_score_, 4),\n",
    "        bayes_cv_tuner.best_params_\n",
    "    ))\n",
    "    \n",
    "    # Save all model results\n",
    "    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n",
    "#     all_models.to_csv(clf_name+\"_cv_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "xgb_skopt = bayes_cv_tuner.fit(X_train, Y_train, callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on training set\n",
    "predictions = xgb_skopt.predict(X_valid)\n",
    "\n",
    "#Print accuracy\n",
    "accuracy = metrics.accuracy_score(predictions,Y_valid)\n",
    "print(\"Validation Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "f1=f1_score(Y_valid, predictions, average=None)\n",
    "print(f'Validation F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stacking(df, [list_of_predictions], metric):\n",
    "    \n",
    "#     if metric=='average':\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=test[train_numeric_predictors].drop(['QuoteNumber','QuoteConversion_Flag'], axis=1)\n",
    "test['QuoteConversion_Flag']=best_model.predict(X_test)\n",
    "test[['QuoteNumber', 'QuoteConversion_Flag']].to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
